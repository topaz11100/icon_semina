
### 슬라이드 1: 제목
발표할 논문 "pFedMoE : Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning"

연합학습 환경에서 데이터 수준의 개인화를 통해 모델 이기종성을 지원하는 방법을 제안

---

### 슬라이드 2: 개요 
연합학습 : 데이터를 중앙으로 모으지 않고 분산된 데이터를 사용하여 공동으로 모델을 훈련하는 방식

연합학습 : 데이터를 중앙 서버에 모으지 않고 여러 클라이언트가 각자의 로컬 데이터로 학습하고 서버에서 가중치를 조정해 공동으로 모델을 훈련하는 머신러닝 방법

FL의 문제 : 

1. 데이터 이질성
2. 시스템 이질성
3. 모델 이질성

---

### 슬라이드 2: 기존 MHPFL 방법 및 한계점
기존의 모델 이질성을 지원하는 개인화 연합학습 방법은 크게 세 가지 접근 방식으로 나뉩니다. 지식 증류를 사용하는 방법, 모델 혼합 방법, 그리고 상호학습을 활용한 방법이다 하지만 이들 방법은 여전히 다음과 같은 문제점을 안고 있습니다:

1. **완전한 모델 이질성 지원 부족**  
2. 데이터 수준의 개인화 부족  
3. 높은 계산 비용과 통신 비용  
4. 데이터 및 모델의 프라이버시 보호 부족

---

### 슬라이드 3: Mixture of Experts 
이를 해결할 수 있는 방법으로 Mixture of Experts, 줄여서 MoE를 사용할 수 있다 MoE는 데이터 샘플이 들어오면 gating 네트워크가 각 Expert 모델의 출력에 가중치를 주고, 이 가중치 합으로 최종 예측을 만드는 방식이다 즉, 데이터의 특성에 따라 가장 잘 맞는 expert 모델을 선택적으로 사용하는 방법이다

---

### 슬라이드 4: 제안 방법 
이 논문에서 제안한 pFedMoE 방법은 MoE 개념을 모델-이질성 개인화 연합학습에 적용한 것이다 각 클라이언트는 공통적으로 사용하는 작은 크기의 Homogeneous feature extractor와 로컬에서만 사용하는 큰 모델인 이질적 feature extractor를 함께 가지며, 이 두 모델의 결과물을 조합하는 gating network를 추가하여 MoE 구조를 형성한다 즉, 글로벌 일반화된 특징과 로컬 개인화된 특성을 데이터 단위로 동적으로 혼합하여 개인화를 강화한다

---

### 슬라이드 5: pFedMoE 구조 설명
더 자세히 살펴보면, 각 클라이언트는 서버로부터 전송받은 글로벌 homogeneous small feature extractor와 로컬의 heterogeneous large feature extractor를 각각 글로벌 및 로컬 expert로 활용한다 여기에 게이팅 네트워크가 각 데이터 샘플마다 두 전문가가 추출한 특징의 가중치를 개인화하여 산출하고, 이를 기반으로 최종 예측을 수행한다 이렇게 얻은 혼합된 표현을 로컬의 prediction header에 입력하여 최종 결과를 도출하고, 모든 모듈을 동시에 학습한다

---

### 슬라이드 6: 이론적 분석
이 논문은 다음과 같은 가정 하에 이론적 분석을 수행하여 pFedMoE가 수렴성을 가짐을 증명했다 Lipschitz smoothness, unbiased gradient, bounded variance 등의 가정을 기반으로, pFedMoE의 local training과 global aggregation이 비볼록 환경에서 일정한 비율로 수렴함을 입증했다

---

### 슬라이드 7: 실험 평가 
실험은 CIFAR-10, CIFAR-100 두 가지 데이터셋을 활용하여 진행했다 실험에서는 데이터 분할 전략으로 Pathological non-IID와 Practical non-IID 두 가지 상황을 설정하여 모델 성능을 비교했다 또한, 모델 균일성 및 모델 이질성 상황 모두에서 기존의 방법들과 비교 실험을 수행했다

---

### 슬라이드 8: 결과 
실험 결과, pFedMoE는 모델-동질성 환경뿐 아니라 이질적인 모델 환경에서도 기존의 방법들보다 더 높은 정확도를 달성했다 특히 CIFAR-10 및 CIFAR-100 데이터셋 모두에서 state-of-the-art 대비 최대 2.80%, 같은 부류의 방법 대비 최대 22.16%의 정확도 향상을 보였다

---

### 슬라이드 9: 개인화 분석 및 시각화
개별 클라이언트의 성능 분석에서도 pFedMoE가 더욱 우수했다 T-SNE 시각화를 통해, pFedMoE가 각 클라이언트의 데이터에 따라 더 명확한 클래스 경계를 생성하며, 데이터 단위에서의 개인화가 잘 이루어지고 있음을 확인할 수 있었다

---

### 슬라이드 8: 계산 비용 및 통신 비용 분석
계산 비용 측면에서 pFedMoE는 로컬 모델과 MoE 구조를 동시에 학습하기 때문에 계산 오버헤드가 낮았다 통신 비용에서는 homogeneous small feature extractor만 서버와 클라이언트 간에 교환하기 때문에 전체 모델을 전송하는 방식보다 적은 비용으로 효율적인 학습이 가능함을 보였다

---

### 슬라이드 9: 강건성 및 민감도 분석
다양한 non-IID 환경에서 pFedMoE는 기존의 방법보다 안정적인 성능을 유지하였으며, gating network의 학습률과 같은 하이퍼파라미터에 민감하지 않아 강건함을 보여주었다

---

### 슬라이드 9: 결론 
마지막으로 결론이다 제안된 pFedMoE 방법은 모델 이질성을 유지하면서도 데이터 단위 개인화를 지원하여 높은 성능을 달성했다 특히 MoE 구조를 통해 모델의 일반화 및 개인화를 데이터 수준에서 동적으로 균형 있게 조절할 수 있었으며, 계산 비용을 줄이고 경쟁력 있는 통신 효율성을 유지할 수 있었다 따라서 pFedMoE는 다양한 실제 환경에서 모델 이질성을 유지하며 효율적으로 개인화된 연합학습을 수행할 수 있는 유망한 방법이다

---