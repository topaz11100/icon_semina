## 제목 Title

https://arxiv.org/abs/2402.01350

pFedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning

## 저자가 생각한 문제점 Problem Statement

### 연합학습에서 나타나는 어려움

1. data or statistical heterogeneity
 - cross device
2. system heterogeneity
 - cross silo
3. model heterogeneity

## 기존 해결책 사례연구 Related Work

이질성 해결 -> Model-Heterogeneous Personalized FL
(이하 MHPFL)
1. MHPFL with Knowledge Distillation (지식증류)
2. MHPFL with Model Mixture (모델 섞기)
3. MHPFL with Mutual Learning

### 그러나

데이터 및 모델의 프라이버시를 보호하면서도 우수한 모델 성능을 유지하고, 통신 및 연산 비용을 낮추는 문제가 여전히 미해결 상태로 남아 있다.

## 정확히 어떤 문제를 해결할 것인가 Research Question

1. 

## 배경지식  Background

1. FL
2. MoE

## 저자의 주장

pFedMoE는 데이터 수준에서의 미세한 개인화를 향상시키면서도 모델 이질성을 지원하는 방식

## 저자가 제안한 문제 해결법 Proposed Solution



## 실험 설계  Experimental Setup



## 실험 결과  Results



## 기존 방법과 비교한 결과 Comparative Analysis



## Ablation study



## 결론 Conclusion



## 향후 논의 Future Directions



## 소감 thoughts